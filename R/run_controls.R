# Run-control builders ------------------------------------------------------

#' Build a tidy grid of prior-quality settings.
#'
#' @param annotation_r2_levels Numeric vector of target annotation R^2 values for causal SNPs.
#' @param inflate_match_levels Numeric vector indicating how strongly to inflate non-causal annotations.
#' @param gamma_shrink_levels Numeric vector of shrinkage slopes for variance-informed runs.
#' @return Tibble with columns `annotation_r2`, `inflate_match`, `gamma_shrink`, and
#'   `prior_quality_id`.
#' @export
prior_quality_grid <- function(annotation_r2_levels = c(0.25, 0.5, 0.75),
                               inflate_match_levels = c(0, 0.5, 1),
                               gamma_shrink_levels = c(0.4, 0.8)) {
  ann_vals <- unique(annotation_r2_levels)
  inflate_vals <- unique(inflate_match_levels)
  gamma_vals <- unique(gamma_shrink_levels)
  if (!any(is.na(gamma_vals))) {
    gamma_vals <- c(gamma_vals, NA_real_)
  }
  grid <- tidyr::expand_grid(
    annotation_r2 = ann_vals,
    inflate_match = inflate_vals,
    gamma_shrink = gamma_vals
  )
  dplyr::mutate(grid, prior_quality_id = dplyr::row_number())
}

#' Construct the full run table for a job.
#'
#' @param use_case_ids Character vector of use-case identifiers.
#' @param L_grid Integer vector of SuSiE/SuSiNE L values.
#' @param y_noise_grid Numeric vector of noise fractions (0-1).
#' @param prior_quality Tibble from [prior_quality_grid()].
#' @param p_star_grid Integer vector for number of causal SNPs.
#' @param seeds Integer vector of RNG seeds.
#' @param data_scenarios Character vector naming the data sources.
#' @return List with elements `scenarios`, `runs`, and `tasks`.
#' @keywords internal
make_run_tables <- function(use_case_ids,
                            L_grid,
                            y_noise_grid,
                            prior_quality,
                            p_star_grid,
                            seeds,
                            data_scenarios,
                            grid_mode = c("full", "minimal")) {
  if (!is.data.frame(prior_quality) ||
      !all(c("annotation_r2", "inflate_match", "gamma_shrink") %in% names(prior_quality))) {
    stop("prior_quality must contain annotation_r2, inflate_match, and gamma_shrink columns.")
  }
  grid_mode <- match.arg(grid_mode)
  use_cases <- resolve_use_cases(use_case_ids)
  if (!nrow(use_cases)) {
    stop("No valid use cases selected.")
  }

  build_full_grid <- function() {
    tidyr::expand_grid(
      data_scenario = data_scenarios,
      L = unique(L_grid),
      y_noise = unique(y_noise_grid),
      p_star = unique(p_star_grid),
      prior_quality_id = prior_quality$prior_quality_id
    ) %>%
      dplyr::left_join(prior_quality, by = "prior_quality_id") %>%
      dplyr::arrange(data_scenario, L, y_noise, p_star, prior_quality_id) %>%
      dplyr::mutate(scenario_id = dplyr::row_number())
  }

  build_minimal_grid <- function() {
    ann_vals <- unique(prior_quality$annotation_r2)
    inflate_vals <- unique(prior_quality$inflate_match)
    gamma_vals <- unique(prior_quality$gamma_shrink)
    if (!length(ann_vals)) {
      ann_vals <- NA_real_
    }
    if (!length(inflate_vals)) {
      inflate_vals <- NA_real_
    }
    if (!length(gamma_vals)) {
      gamma_vals <- NA_real_
    }
    values <- list(
      data_scenario = unique(data_scenarios),
      L = unique(L_grid),
      y_noise = unique(y_noise_grid),
      p_star = unique(p_star_grid),
      annotation_r2 = ann_vals,
      inflate_match = inflate_vals,
      gamma_shrink = gamma_vals
    )
    lengths <- vapply(values, length, integer(1))
    n_rows <- max(lengths)
    tibble::tibble(
      data_scenario = rep_len(values$data_scenario, n_rows),
      L = rep_len(values$L, n_rows),
      y_noise = rep_len(values$y_noise, n_rows),
      p_star = rep_len(values$p_star, n_rows),
      annotation_r2 = rep_len(values$annotation_r2, n_rows),
      inflate_match = rep_len(values$inflate_match, n_rows),
      gamma_shrink = rep_len(values$gamma_shrink, n_rows),
      prior_quality_id = seq_len(n_rows)
    ) %>%
      dplyr::mutate(scenario_id = seq_along(data_scenario))
  }

  scenarios <- if (grid_mode == "minimal") {
    build_minimal_grid()
  } else {
    build_full_grid()
  }

  seed_values <- unique(seeds)
  if (!length(seed_values)) {
    stop("At least one seed must be supplied.")
  }
  if (grid_mode == "minimal") {
    seed_values <- seed_values[1]
  }

  runs <- tidyr::expand_grid(
    scenario_id = scenarios$scenario_id,
    use_case_id = use_cases$use_case_id,
    seed = seed_values
  ) %>%
    dplyr::left_join(scenarios, by = "scenario_id") %>%
    dplyr::left_join(
      dplyr::select(use_cases, use_case_id, requires_prior_quality, mu_strategy, sigma_strategy),
      by = "use_case_id"
    )

  runs <- runs %>%
    dplyr::mutate(
      needs_annotation = requires_prior_quality,
      needs_gamma = requires_prior_quality & sigma_strategy == "functional"
    ) %>%
    dplyr::filter(
      !needs_annotation |
        (needs_gamma & !is.na(gamma_shrink)) |
        (!needs_gamma & is.na(gamma_shrink))
    )

  default_scenarios <- scenarios %>%
    dplyr::group_by(data_scenario, L, y_noise, p_star) %>%
    dplyr::summarise(default_scenario_id = dplyr::first(scenario_id), .groups = "drop")

  runs <- runs %>%
    dplyr::left_join(default_scenarios,
      by = c("data_scenario", "L", "y_noise", "p_star")
    ) %>%
    dplyr::mutate(
      scenario_id = dplyr::if_else(
        requires_prior_quality,
        scenario_id,
        default_scenario_id
      ),
      prior_quality_id = dplyr::if_else(
        requires_prior_quality,
        prior_quality_id,
        as.integer(NA)
      ),
      annotation_r2 = dplyr::if_else(
        requires_prior_quality,
        annotation_r2,
        as.numeric(NA)
      ),
      inflate_match = dplyr::if_else(
        requires_prior_quality,
        inflate_match,
        as.numeric(NA)
      ),
      gamma_shrink = dplyr::if_else(
        needs_gamma,
        gamma_shrink,
        as.numeric(NA)
      )
    ) %>%
    dplyr::select(-default_scenario_id, -needs_annotation, -needs_gamma, -mu_strategy, -sigma_strategy) %>%
    dplyr::distinct() %>%
    dplyr::arrange(scenario_id, use_case_id, seed) %>%
    dplyr::mutate(run_id = dplyr::row_number())

  scenarios <- dplyr::semi_join(scenarios, runs, by = "scenario_id")

  list(
    scenarios = scenarios,
    runs = runs,
    use_cases = use_cases
  )
}

#' Attach task identifiers to the run table.
#'
#' @param runs Run tibble from [make_run_tables()].
#' @param runs_per_task Integer number of runs assigned to each SLURM task.
#' @param shuffle_seed Optional seed for reproducible shuffling (default: NULL for random).
#' @return Tibble with `task_id` and `shuffled_order` columns added, and supporting task summary.
#' @keywords internal
assign_task_ids <- function(runs, runs_per_task, shuffle_seed = NULL) {
  if (runs_per_task < 1) {
    stop("runs_per_task must be >= 1")
  }

  n_runs <- nrow(runs)

  # Create shuffled order
  if (!is.null(shuffle_seed)) {
    set.seed(shuffle_seed)
  }
  shuffled_idx <- sample(n_runs)

  # Assign task_ids based on shuffled order
  runs_shuffled <- runs %>%
    dplyr::mutate(shuffled_order = shuffled_idx) %>%
    dplyr::arrange(shuffled_order) %>%
    dplyr::mutate(task_id = ((dplyr::row_number() - 1L) %/% as.integer(runs_per_task)) + 1L) %>%
    dplyr::arrange(run_id)  # unshuffle back to original run_id order

  tasks_summary <- runs_shuffled %>%
    dplyr::group_by(task_id) %>%
    dplyr::summarise(runs_per_task = dplyr::n(), .groups = "drop") %>%
    dplyr::arrange(task_id)

  list(runs = runs_shuffled, tasks = tasks_summary)
}

#' Create an in-memory job configuration.
#'
#' @param job_name Character scalar.
#' @param use_case_ids Character vector of selected use cases.
#' @param L_grid Integer vector of L values.
#' @param y_noise_grid Numeric vector of noise fractions.
#' @param prior_quality Tibble with prior noise settings.
#' @param p_star_grid Integer vector.
#' @param seeds Integer vector of RNG seeds.
#' @param data_scenarios Character vector.
#' @param runs_per_task Integer number of runs assigned to each SLURM task.
#' @param email Notification email address.
#' @param output_root Root directory for outputs (default `output`).
#' @param credible_set_rho Credible set cumulative PIP threshold.
#' @param purity_threshold Minimum purity to keep CS in filtered summary.
#' @param anneal_settings Named list for tempering runs.
#' @param model_average_settings Named list for model averaging runs.
#'
#' @return Job configuration list ready to serialize.
#' @export
make_job_config <- function(job_name,
                            HPC = FALSE,
                            time = "02:59:00",
                            mem = "2G",
                            use_case_ids,
                            L_grid,
                            y_noise_grid,
                            prior_quality,
                            p_star_grid,
                            seeds,
                            data_scenarios = "simulation_n3",
                            runs_per_task = 150,
                            email = "mgc5166@psu.edu",
                            output_root = "output",
                            credible_set_rho = 0.95,
                            purity_threshold = 0.5,
                            grid_mode = c("full", "minimal"),
                            # NEW: sharding + padding controls (can be overridden per call)
                            shard_size_output = 1000L,  # 0 disables sharding for slurm_output
                            shard_size_prints = 1000L,  # 0 disables sharding for slurm_prints
                            pad_width = NULL,           # NULL => renderer auto-computes from n_tasks
                            anneal_settings = list(
                              anneal_start_T = 5,
                              anneal_schedule_type = "geometric",
                              anneal_burn_in = 5
                            ),
                            model_average_settings = list(
                              n_inits = 5,
                              init_sd = 0.05
                            )) {

  grid_mode <- match.arg(grid_mode)

  tables <- make_run_tables(
    use_case_ids = use_case_ids,
    L_grid = L_grid,
    y_noise_grid = y_noise_grid,
    prior_quality = prior_quality,
    p_star_grid = p_star_grid,
    seeds = seeds,
    data_scenarios = data_scenarios,
    grid_mode = grid_mode
  )

  runs_tasks <- assign_task_ids(tables$runs, runs_per_task = runs_per_task)

  list(
    job = list(
      name = job_name,
      HPC = HPC,
      email = email,
      created_at = timestamp_utc(),
      runs_per_task = runs_per_task,
      credible_set_rho = credible_set_rho,
      purity_threshold = purity_threshold,
      compute = list(
        anneal = anneal_settings,
        model_average = model_average_settings
      ),
      slurm = list(
        time = time,
        mem = mem,
        cpus_per_task = 1,
        partition = NULL,
        # NEW: plumbed through for render_slurm_script()
        shard_size_output = as.integer(shard_size_output),
        shard_size_prints = as.integer(shard_size_prints),
        pad_width = if (is.null(pad_width)) NULL else as.integer(pad_width)
      )
    ),
    paths = list(
      output_root = output_root,
      run_history_dir = file.path(output_root, "run_history", job_name),
      temp_dir = file.path(output_root, "temp", job_name),
      slurm_output_dir = file.path(output_root, "slurm_output"),
      slurm_prints_dir = file.path(output_root, "slurm_prints"),
      slurm_scripts_dir = file.path(output_root, "slurm_scripts")
    ),
    tables = list(
      scenarios = tables$scenarios,
      runs = runs_tasks$runs,
      tasks = runs_tasks$tasks,
      use_cases = tables$use_cases
    )
  )
}
#' Write job configuration and scripts to disk.
#'
#' @param job_config Output of [make_job_config()].
#' @param run_task_script Path to the general task-runner script.
#' @return List with paths to the serialized artifacts.
#' @export
write_job_artifacts <- function(job_config,
                                run_task_script) {
  paths <- job_config$paths
  ensure_dir(paths$output_root)
  ensure_dir(paths$slurm_output_dir)
  ensure_dir(paths$slurm_prints_dir)
  ensure_dir(paths$slurm_scripts_dir)

  # Clear temp directory and create fresh
  unlink(paths$temp_dir, recursive = TRUE)
  ensure_dir(paths$temp_dir)

  job_json_path <- file.path(paths$temp_dir, "job_config.json")
  jsonlite::write_json(
    job_config,
    path = job_json_path,
    auto_unbox = TRUE,
    digits = NA,
    pretty = TRUE
  )

  run_table_path <- file.path(paths$temp_dir, "run_table.csv")
  readr::write_csv(job_config$tables$runs, run_table_path)

  scenario_path <- file.path(paths$temp_dir, "scenario_table.csv")
  readr::write_csv(job_config$tables$scenarios, scenario_path)

  use_case_path <- file.path(paths$temp_dir, "use_cases.csv")
  readr::write_csv(job_config$tables$use_cases, use_case_path)

  task_path <- file.path(paths$temp_dir, "task_table.csv")
  readr::write_csv(job_config$tables$tasks, task_path)

  slurm_path <- file.path(paths$slurm_scripts_dir, paste0(job_config$job$name, ".slurm"))
  writeLines(
    render_slurm_script(job_config, run_task_script = run_task_script),
    con = slurm_path
  )

  list(
    job_config = job_json_path,
    run_table = run_table_path,
    scenarios = scenario_path,
    use_cases = use_case_path,
    tasks = task_path,
    slurm_script = slurm_path
  )
}

#' Render a SLURM submission script string.
#'
#' @param job_config Job configuration list.
#' @param run_task_script Path to `run_task.R`.
#' @return Character vector of script lines.
#' @keywords internal
render_slurm_script <- function(job_config, run_task_script) {
  job    <- job_config$job
  paths  <- job_config$paths
  tasks  <- job_config$tables$tasks
  n_tasks <- nrow(tasks)
  slurm  <- job$slurm

  partition_line <- if (!is.null(slurm$partition)) {
    paste0("#SBATCH --partition=", slurm$partition)
  } else {
    NULL
  }

  hpc_setup <- if (isTRUE(job$HPC)) {
    c(
      "module load r",
      "",
      'export R_LIBS_USER="/storage/home/mgc5166/R/x86_64-pc-linux-gnu-library/4.3"',
      ""
    )
  } else {
    NULL
  }

  # ---- padding & sharding config ----
  pad_width <- max(4L, ceiling(log10(max(1L, n_tasks))))  # e.g., 120000 -> 6
  shard_size_output <- job$slurm$shard_size_output %||% 1000L  # for slurm_output
  shard_size_prints <- job$slurm$shard_size_prints %||% 1000L  # for slurm_prints

  script <- c(
    "#!/bin/bash",
    sprintf("#SBATCH --job-name=%s", job$name),
    sprintf("#SBATCH --array=1-%d", n_tasks),
    sprintf("#SBATCH --time=%s", slurm$time),
    sprintf("#SBATCH --mem=%s", slurm$mem),
    sprintf("#SBATCH --cpus-per-task=%s", slurm$cpus_per_task),
    sprintf("#SBATCH --mail-user=%s", job$email),
    "#SBATCH --mail-type=BEGIN,END,FAIL",
    partition_line,
    "# Log to /dev/null; we'll redirect ourselves below.",
    "#SBATCH --output=/dev/null",
    "#SBATCH --error=/dev/null",
    "",
    "set -euo pipefail",
    "",
    sprintf('JOB_ROOT="%s"', normalizePath(paths$output_root, winslash = "/", mustWork = FALSE)),
    sprintf('CONFIG_PATH="%s"', normalizePath(file.path(paths$temp_dir, "job_config.json"), winslash = "/", mustWork = FALSE)),
    sprintf('RUN_TASK_SCRIPT="%s"', normalizePath(run_task_script, winslash = "/", mustWork = FALSE)),
    sprintf('SLURM_PRINTS_BASE="%s"', normalizePath(paths$slurm_prints_dir, winslash = "/", mustWork = FALSE)),
    sprintf('SLURM_OUTPUT_BASE="%s"', normalizePath(paths$slurm_output_dir, winslash = "/", mustWork = FALSE)),
    sprintf('TEMP_DIR="%s"', normalizePath(paths$temp_dir, winslash = "/", mustWork = FALSE)),
    sprintf('RUN_HISTORY_BASE="%s"', normalizePath(file.path(paths$output_root, "run_history"), winslash = "/", mustWork = FALSE)),
    "",
    "# --- identifiers (stable across Slurm versions) ---",
    'JOBNAME="${SLURM_JOB_NAME}"',
    'PARENT_ID="${SLURM_ARRAY_JOB_ID:-$SLURM_JOB_ID}"   # parent array ID if array, else job id',
    'TASK_ID="${SLURM_ARRAY_TASK_ID:-0}"',
    sprintf('PAD=%d', as.integer(pad_width)),
    sprintf('SHARD_SIZE_OUTPUT=%d', as.integer(shard_size_output)),
    sprintf('SHARD_SIZE_PRINTS=%d', as.integer(shard_size_prints)),
    "",
    "# --- compute shard dir for PRINTS ---",
    'if [ "${SHARD_SIZE_PRINTS}" -gt 0 ]; then',
    '  TI="${TASK_ID}"; if [ "${TI}" -le 0 ]; then TI=1; fi',
    '  SHARD_IDX_PRINTS=$(( (TI - 1) / SHARD_SIZE_PRINTS ))',
    '  SHARD_DIR_PRINTS="$(printf "shard-%03d" "${SHARD_IDX_PRINTS}")"',
    '  PRINTS_DIR="${SLURM_PRINTS_BASE}/${JOBNAME}/${PARENT_ID}/${SHARD_DIR_PRINTS}"',
    'else',
    '  PRINTS_DIR="${SLURM_PRINTS_BASE}/${JOBNAME}/${PARENT_ID}"',
    'fi',
    'mkdir -p "${PRINTS_DIR}"',
    "",
    "# Redirect logs (after dirs exist) â€” one file per task",
    'exec >"${PRINTS_DIR}/${TASK_ID}.out" 2>"${PRINTS_DIR}/${TASK_ID}.err"',
    'echo "[$(date -Is)] Starting task ${TASK_ID} for job ${JOBNAME} (parent ${PARENT_ID})"',
    "",
    "# --- export job info for R to compute run-specific output dirs ---",
    'export SUSINE_JOB_NAME="${JOBNAME}"',
    'export SUSINE_PARENT_ID="${PARENT_ID}"',
    "# --- site/module setup ---",
    hpc_setup,
    "",
    "# --- task 1 pre-creates all shard directories and copies run_history ---",
    'if [ "${TASK_ID}" = "1" ]; then',
    '  FINAL_HISTORY_DIR="${RUN_HISTORY_BASE}/${JOBNAME}/${PARENT_ID}"',
    '  mkdir -p "${FINAL_HISTORY_DIR}"',
    '  cp "${TEMP_DIR}"/* "${FINAL_HISTORY_DIR}/"',
    '  echo "[$(date -Is)] Task 1: copied run_history from temp to ${FINAL_HISTORY_DIR}"',
    '  ',
    '  # Pre-create all shard directories to avoid race conditions',
    '  SLURM_OUTPUT_JOB_DIR="${SLURM_OUTPUT_BASE}/${JOBNAME}/${PARENT_ID}"',
    '  if [ "${SHARD_SIZE_OUTPUT}" -gt 0 ]; then',
    '    # Read max run_id from run_table.csv (header + data)',
    '    MAX_RUN_ID=$(tail -1 "${FINAL_HISTORY_DIR}/run_table.csv" | cut -d, -f1)',
    '    MAX_SHARD=$(( (MAX_RUN_ID - 1) / SHARD_SIZE_OUTPUT ))',
    '    for ((i=0; i<=MAX_SHARD; i++)); do',
    '      SHARD_DIR="$(printf "shard-%03d" "$i")"',
    '      mkdir -p "${SLURM_OUTPUT_JOB_DIR}/${SHARD_DIR}"',
    '    done',
    '    echo "[$(date -Is)] Task 1: pre-created shards 0-${MAX_SHARD} for ${MAX_RUN_ID} runs"',
    '  fi',
    'fi',
    "",
    "# --- run task ---",
    'Rscript "$RUN_TASK_SCRIPT" \\',
    '  --job-name "$JOBNAME" \\',
    '  --task-id "$TASK_ID" \\',
    '  --job-root "$JOB_ROOT" \\',
    '  --config-path "$CONFIG_PATH"',
    "",
    'echo "[$(date -Is)] Completed task ${TASK_ID}"'
  )
  script[!is.na(script)]
}

#' Summarise expected run counts per use case and scenario.
#'
#' @param job_config Job configuration list.
#' @return Tibble summarising runs.
#' @export
summarise_job_config <- function(job_config) {
  runs <- job_config$tables$runs
  runs %>%
    dplyr::group_by(
      use_case_id,
      L,
      y_noise,
      p_star,
      annotation_r2,
      inflate_match,
      gamma_shrink
    ) %>%
    dplyr::summarise(n_runs = dplyr::n(), .groups = "drop")
}
