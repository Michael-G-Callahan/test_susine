---
title: "SuSiNE Simulation Results Collection"
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
editor_options:
  markdown:
    wrap: 72
---

Summary: Requires a completed simulation run with run_history and task outputs. This notebook aggregates task outputs, optionally builds a parquet `snps_dataset`, and writes confusion matrices plus dataset/multimodal metrics for downstream analysis.

# Section 1: Setup (Always Run First)
 
Load libraries, specify job identifiers, and establish paths. This section
must be run before either Section 2 or Section 3.

```{r setup, message=FALSE}
here::i_am("vignettes/simulation pipeline/collect_results_workbook.Rmd")
library(here)
here()

library(devtools)
devtools::load_all(".")

library(dplyr)
library(tidyr)
library(readr)
library(knitr)
library(purrr)
library(fs)
library(arrow)
```

## 1.1 Specify the Job

```{r job-inputs}
job_name <- "prior_default_testing"
parent_job_id <- "47128217"
output_root <- here("output")

# Toggle to skip writing the aggregated snps_dataset parquet folder.
# Set to TRUE when you only need confusion matrices and want to reduce I/O.
skip_snps_dataset_aggregation <- FALSE
```

## 1.2 Resolve Paths and Load Run Table

```{r load-paths-and-run-table}
# Resolve all standard paths
paths <- test_susine::resolve_job_paths(
  job_name = job_name,
  parent_job_id = parent_job_id,
  output_root = output_root
)

# Validate that run_history exists (required for all workflows)
if (!dir.exists(paths$run_history_dir)) {
  stop("Run history directory not found: ", paths$run_history_dir)
}

# Load run table (needed by both Section 2 and Section 3)
run_table <- readr::read_csv(
  file.path(paths$run_history_dir, "run_table.csv"),
  show_col_types = FALSE
)

# Use cases path (for labels in confusion matrices)
use_cases_path <- file.path(paths$run_history_dir, "use_cases.csv")

cat("Job:", job_name, "/", parent_job_id, "\n")
cat("Run history dir:", paths$run_history_dir, "\n")
cat("Aggregated dir:", paths$aggregated_dir, "\n")
cat("SNPs dataset dir:", paths$snps_dataset_dir, "\n")
cat("Loaded run_table with", nrow(run_table), "runs\n")
```

---
---

# Section 2: Staging -> Aggregated (Auto-skip if already done)

This section processes raw task outputs into aggregated files.
It will **automatically skip** if aggregated outputs already exist.
Set `skip_snps_dataset_aggregation = TRUE` to skip writing the parquet
snps_dataset while still aggregating the metrics CSVs.

```{r check-aggregation-status}
staging_exists <- dir.exists(paths$staging_dir)
snps_dataset_exists <- dir.exists(paths$snps_dataset_dir) && 
                       length(list.files(paths$snps_dataset_dir, recursive = TRUE)) > 0

cat("Task output directory exists:", staging_exists, "\n")
cat("Aggregated SNPs dataset exists:", snps_dataset_exists, "\n")

# Determine whether to run aggregation
run_aggregation <- staging_exists && (!snps_dataset_exists || skip_snps_dataset_aggregation)

if (snps_dataset_exists && !skip_snps_dataset_aggregation) {
  message("Aggregated outputs already exist. Skipping Section 2.")
} else if (!staging_exists) {
  stop("Neither task outputs nor aggregated outputs found. Cannot proceed.")
} else {
  message(if (skip_snps_dataset_aggregation) "Task outputs found; aggregating metrics (skipping snps_dataset parquet)." else "Task outputs found but not yet aggregated. Will run aggregation.")
}
```

## 2.1 Index Staging Files

```{r index-staging}
if (run_aggregation) {
  staging_index <- test_susine::index_staging_outputs(
    job_name = job_name,
    parent_job_id = parent_job_id,
    output_root = output_root
  )
  
  cat("Discovered", nrow(staging_index), "task output files across",
      length(unique(staging_index$task_id)), "tasks\n")
} else {
  message("Skipped: aggregation not needed.")
}
```

## 2.2 Validate Staging Files

```{r validate-staging}
if (run_aggregation) {
  validation <- test_susine::validate_staging_outputs(staging_index)
  
  if (nrow(validation)) {
    fail_count <- sum(!validation$ok, na.rm = TRUE)
    cat("Validation failures:", fail_count, "\n")
    if (fail_count > 0) {
      print(dplyr::filter(validation, !ok))
      stop("Fix unreadable task output files before aggregating.")
    }
  } else {
    cat("No task output files found to validate.\n")
  }
} else {
  message("Skipped: aggregation not needed.")
}
```

## 2.3 Aggregate to Job-Level Outputs

```{r aggregate}
if (run_aggregation) {
  dir.create(paths$aggregated_dir, showWarnings = FALSE, recursive = TRUE)
  
  agg <- test_susine::aggregate_staging_outputs(
    job_name = job_name,
    parent_job_id = parent_job_id,
    output_root = output_root,
    validate = FALSE,  # already validated above
    output_dir = paths$aggregated_dir,
    write_snps_dataset = !skip_snps_dataset_aggregation
  )
  
  cat("Aggregated outputs written to:", agg$output_dir, "\n")
  
  if (!skip_snps_dataset_aggregation && dir.exists(paths$snps_dataset_dir)) {
    snp_files <- list.files(paths$snps_dataset_dir, pattern = "\\.parquet$", recursive = TRUE)
    cat("SNP dataset fragments:", length(snp_files), "\n")
  }
} else {
  message("Skipped: aggregation not needed.")
}
```

## 2.4 (Optional) Delete Staging - MANUAL ONLY

**This chunk is disabled by default for safety.** Set `eval=TRUE` manually
if you want to delete staging files after confirming aggregation.

```{r cleanup-staging, eval=FALSE}
# DANGER: Set eval=TRUE only after verifying aggregation is complete
if (dir.exists(paths$staging_dir)) {
  unlink(paths$staging_dir, recursive = TRUE)
  cat("Deleted task output directory:", paths$staging_dir, "\n")
} else {
  cat("Task output directory already deleted.\n")
}
```

---
---

# Section 3: Build Pre-aggregated Confusion Matrices

Create compact summary tables for Power/FDR/AUPRC analysis. This section
prefers `confusion_bins` when available (aggregated or staging), and
falls back to the aggregated SNPs dataset or staging parquet fragments
when bins are not present.

**Prerequisites:** 
- Section 1 must have been run (to load paths and run_table).
- Either `confusion_bins` (aggregated or staging), or an aggregated SNPs
  dataset at `paths$snps_dataset_dir`, or staging parquet fragments.

## 3.1 Check Confusion Bins / SNPs Dataset Exists

```{r check-snps-dataset}
confusion_bins_path <- file.path(paths$aggregated_dir, "confusion_bins.csv")
confusion_bins_exists <- file.exists(confusion_bins_path)
staging_confusion_files <- list.files(
  paths$staging_dir,
  pattern = "_confusion_bins\\.csv$",
  recursive = TRUE,
  full.names = TRUE
)

snps_dataset_exists <- dir.exists(paths$snps_dataset_dir) &&
                       length(list.files(paths$snps_dataset_dir, recursive = TRUE)) > 0
staging_snp_files <- list.files(
  paths$staging_dir,
  pattern = "_snps\\.parquet$",
  recursive = TRUE,
  full.names = TRUE
)

cat("Confusion bins (aggregated) exists:", confusion_bins_exists, "\n")
cat("Task confusion bins available:", length(staging_confusion_files), "\n")
cat("SNPs dataset exists:", snps_dataset_exists, "\n")
cat("Task SNP parquet fragments available:", length(staging_snp_files), "\n")

if (!confusion_bins_exists && !length(staging_confusion_files) &&
    !snps_dataset_exists && !length(staging_snp_files)) {
  stop("No confusion bins, SNPs dataset, or task parquet fragments found. Run Section 2 first.")
}

if (snps_dataset_exists) {
  partitions <- list.dirs(paths$snps_dataset_dir, recursive = FALSE, full.names = FALSE)
  cat("Partitions found:", length(partitions), "\n")
  if (length(partitions) <= 10) {
    cat(paste(" ", partitions, collapse = "\n"), "\n")
  }
}
```

## 3.2 Configure Confusion Matrix Parameters

```{r confusion-config}
# PIP bucket width (0.01 = 100 buckets from 0 to 1)
pip_bucket_width <- 0.01

# Grouping variables for the detailed confusion matrix
# This will create one row per (grouping combo x pip_threshold)
detailed_grouping_vars <- c(
  "use_case_id", "variant_type", "agg_method",
  "p_star", "L", "y_noise",
  "annotation_r2", "inflate_match", "gamma_shrink",
  "annotation_scale", "sigma_0_2_scalar"
)

# Grouping variables for the dataset-level confusion matrix
dataset_grouping_vars <- c("matrix_id", "use_case_id", "variant_type", "agg_method")
```

## 3.3 Build Detailed Confusion Matrix (by Use Case + Parameters)

```{r build-confusion-detailed, message=FALSE}
detailed_output_path <- file.path(paths$aggregated_dir, "confusion_matrix_detailed.csv")
use_confusion_bins <- confusion_bins_exists || length(staging_confusion_files) > 0
stream_confusion <- !use_confusion_bins && !snps_dataset_exists

confusion_detailed <- test_susine::build_confusion_matrix(
  confusion_bins_path = if (confusion_bins_exists) confusion_bins_path else NULL,
  confusion_bins_files = if (!confusion_bins_exists && length(staging_confusion_files)) staging_confusion_files else NULL,
  snps_dataset_path = if (!use_confusion_bins && snps_dataset_exists) paths$snps_dataset_dir else NULL,
  snp_files = if (!use_confusion_bins && !snps_dataset_exists) staging_snp_files else NULL,
  run_table = run_table,
  grouping_vars = detailed_grouping_vars,
  pip_bucket_width = pip_bucket_width,
  use_cases_path = use_cases_path,
  output_path = detailed_output_path,
  stream = stream_confusion
)

cat("\nDetailed confusion matrix:\n")
cat("  Rows:", format(nrow(confusion_detailed), big.mark = ","), "\n")
cat("  Columns:", ncol(confusion_detailed), "\n")
cat("  Size in memory:", format(object.size(confusion_detailed), units = "MB"), "\n")
cat("  Saved to:", detailed_output_path, "\n")
```

## 3.4 Build Dataset-Level Confusion Matrix (by matrix_id only)

```{r build-confusion-dataset, message=FALSE}
dataset_output_path <- file.path(paths$aggregated_dir, "confusion_matrix_by_dataset.csv")

confusion_by_dataset <- test_susine::build_confusion_matrix(
  confusion_bins_path = if (confusion_bins_exists) confusion_bins_path else NULL,
  confusion_bins_files = if (!confusion_bins_exists && length(staging_confusion_files)) staging_confusion_files else NULL,
  snps_dataset_path = if (!use_confusion_bins && snps_dataset_exists) paths$snps_dataset_dir else NULL,
  snp_files = if (!use_confusion_bins && !snps_dataset_exists) staging_snp_files else NULL,
  run_table = run_table,
  grouping_vars = dataset_grouping_vars,
  pip_bucket_width = pip_bucket_width,
  use_cases_path = NULL,  
  output_path = dataset_output_path,
  stream = stream_confusion
)

cat("\nDataset-level confusion matrix:\n")
cat("  Rows:", format(nrow(confusion_by_dataset), big.mark = ","), "\n")
cat("  Unique matrix_ids:", dplyr::n_distinct(confusion_by_dataset$matrix_id), "\n")
cat("  Size in memory:", format(object.size(confusion_by_dataset), units = "KB"), "\n")
cat("  Saved to:", dataset_output_path, "\n")
```

## 3.5 Summary

```{r summary}
cat("\n=== Output Files ===\n")
output_files <- list.files(paths$aggregated_dir, pattern = "\\.(csv|parquet)$", recursive = FALSE)
for (f in output_files) {
  fpath <- file.path(paths$aggregated_dir, f)
  fsize <- file.size(fpath)
  cat(sprintf("  %s (%s)\n", f, format(structure(fsize, class = "object_size"), units = "auto")))
}

if (dir.exists(paths$snps_dataset_dir)) {
  snp_size <- sum(file.size(list.files(paths$snps_dataset_dir, recursive = TRUE, full.names = TRUE)))
  cat(sprintf("  snps_dataset/ (%s total)\n", format(structure(snp_size, class = "object_size"), units = "auto")))
}
```

---
---

# Section 4: Inspect Dataset + Multimodal Metrics (Optional)

Load the aggregated dataset-level and multimodal metrics if they were
written during the run.

```{r load-metrics}
dataset_metrics_path <- file.path(paths$aggregated_dir, "dataset_metrics.csv")
multimodal_metrics_path <- file.path(paths$aggregated_dir, "multimodal_metrics.csv")

if (file.exists(dataset_metrics_path)) {
  dataset_metrics <- readr::read_csv(dataset_metrics_path, show_col_types = FALSE)
  cat("Loaded dataset_metrics:", nrow(dataset_metrics), "rows\n")
} else {
  message("dataset_metrics.csv not found.")
}

if (file.exists(multimodal_metrics_path)) {
  multimodal_metrics <- readr::read_csv(multimodal_metrics_path, show_col_types = FALSE)
  cat("Loaded multimodal_metrics:", nrow(multimodal_metrics), "rows\n")
} else {
  message("multimodal_metrics.csv not found.")
}
```

