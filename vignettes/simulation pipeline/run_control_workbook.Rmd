---
title: "SuSiNE Simulation Run Control"
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
editor_options: 
  markdown: 
    wrap: 72
---

Summary: Builds SLURM-ready simulation control files from a parameter grid. It previews run/task tables and writes job artifacts (config JSON + SLURM script) used by `inst/scripts/run_task.R`.

```{r setup, message=FALSE}
here::i_am("vignettes/run_control_workbook.Rmd")
library(here)
here()

library(devtools)
devtools::load_all(".")
devtools::load_all("../../susine")

library(dplyr)
library(tidyr)
library(readr)
library(knitr)
```

# Guided Workflow

This workbook builds the control files needed to launch a simulation
sweep on SLURM. It walks through the inputs you must supply, previews
the resulting run matrix, and (optionally) writes the job configuration,
array script, and helper R script.

The generated artifacts follow the repository layout:

-   output/temp/<job_name> - temporary staging for run configuration (cleared each write).
-   output/run_history/<job_name>/<parent_job_id> - permanent immutable record copied from temp after first SLURM task.
-   output/slurm_scripts/<job_name>.slurm - submission script.
-   inst/scripts/run_task.R - task runner referenced by each SLURM array task.
-   During execution, results land in task-local staging files at output/slurm_output/<job_name>/<parent_job_id>/combined/staging/task-*/flush-*_*.
-   SLURM stdout/stderr are re-homed to output/slurm_prints/<job_name>/<parent_job_id>/....

Each completed run now writes a compact `snps.parquet` file that merges
the combined PIPs with the causal truth table so downstream routines can
work off a single artifact. Flip `write_legacy_snp_csv = TRUE` inside
`job_inputs` if you still need the historical `pip.csv`/`truth.csv`
pairs for ad hoc debugging. If downstream confusion matrices are the
only parquet consumer, you can now skip writing the aggregated
`snps_dataset` during collection to save time/space.

Set `verbose_file_output = FALSE` to suppress the per-run directories and
stream metrics into task-local CSV/Parquet flush files during the
simulation (recommended for large sweeps with strict filesystem quotas).


# 1. Specify the Job

Edit the list below to reflect the sweep you want to perform. Every
section is commented; update values as needed before knitting or running
the chunks interactively. Switch `grid_mode` to `"minimal"` for a
smoke-test grid that touches each parameter value once instead of the
full Cartesian product (only the first seed in `seeds` is used in that
mode).

```{r job-inputs}
job_inputs <- list(
  # REQUIRED: unique slug for this run
  job_name = "prior_default_testing",
  
  HPC = TRUE,
  
  mem = "8G",
  time = "23:59:59",

  # Email used for SLURM notifications
  email = "mgc5166@psu.edu",

  # Root folder that already contains run_history/, slurm_output/, slurm_prints/
  output_root = here("output"),

  # Grid mode: "full" for Cartesian product, "minimal" for lightweight smoke tests
  grid_mode = "full",

  # Which predefined model variants to include (see use_case_catalog())
  use_cases = c("a_i", "b_i", "b_ii"),

  # Simulation settings (vectorised; full factorial is taken)
  L_grid = c(10),
  y_noise = c(0.6, 0.8, 0.9, 0.95),
  p_star_grid = c(1, 2, 3, 4, 5, 10, 20),
  pair_L_p_star = FALSE,
 
  # Prior quality grid parameters
  annotation_r2 = c(0.2, 0.4, 0.6, 0.8),
  inflate_match = c(0.5, 1, 1.5),
  gamma_shrink = c(0.4, 0.8),
  # Scalar multipliers for functional prior means (mu_0)
  annotation_scale = c(1),
  
  # sigma_0_2 prior variance scalars to test (only for naive sigma use cases)
  # Formats: "X" = X * var(y), "X/L" = X/L * var(y)
  # Set to NULL to use default (1/L * var_y)
  sigma_0_2_scalars = c(
    "0.1",    # 0.1 * var_y (small, susieR-like)
    "0.2",    # 0.2 * var_y (susieR default)
    "0.4"     # 0.4 * var_y (larger)
  ),

  # Seeds per design matrix (each sampled X receives these many phenotype draws)
  seeds = 1:3,

  # Number of runs to assign to each SLURM array task
  # (~2112 total runs / 150 per task = ~14 tasks, well under SLURM QoS limits)
  runs_per_task = 3969, #5,
  # Number of runs to accumulate before flushing task-local outputs
  buffer_flush_interval = 300,

  # Data scenarios: "simulation_n3" for the built-in toy data, or scenario_{1-4}
  # for the sampled genotype panels (see vignette for details)
  data_scenarios = c("scenario_1"),

  # Repository root + sampled scenario manifest (used for scenario_1-4 matrices)
  repo_root = here(),
  sampled_scenario_summary = here("data", "sampled_simulated_genotypes", "scenario_sampling_summary.csv"),

  # Credible-set evaluation knobs
  credible_set_rho = 0.95,
  purity_threshold = 0.50,
  verbose_file_output = FALSE,
  write_legacy_snp_csv = FALSE
)
```

Set `pair_L_p_star = TRUE` to lock `L_grid` and `p_star_grid` together (only identical
values are paired when the full grid is built); flip it to `FALSE` if you want the
complete Cartesian product again.

Double-check that `job_name` is meaningful and unique — it determines
where the job history and outputs are stored.

When you choose one of the sampled data scenarios (`scenario_1`–`scenario_4`),
the grid will automatically include every available design matrix (one per
gene) from `data/sampled_simulated_genotypes/…`. The `seeds` vector now
controls **phenotype replicates per matrix** rather than per scenario, so
in the example above we will simulate five phenotypes for each extracted
`X`. The legacy `simulation_n3` scenario still behaves as before with a
single built-in matrix.

If you request any of the sampled scenarios, make sure the
`sampled_scenario_summary` path points to the CSV written by the genotype
sampling workbook; it enumerates every matrix/manifest pair we just
generated.

# 2. Build the Run Tables

Convert the user inputs into the tidy tables that power the task runner.

```{r build-config}
prior_quality <- prior_quality_grid(
  annotation_r2_levels = job_inputs$annotation_r2,
  inflate_match_levels = job_inputs$inflate_match,
  gamma_shrink_levels = job_inputs$gamma_shrink
)

job_config <- make_job_config(
  job_name = job_inputs$job_name,
  HPC = job_inputs$HPC,
  time = job_inputs$time,
  mem = job_inputs$mem,
  use_case_ids = job_inputs$use_cases,
  L_grid = job_inputs$L_grid,
  y_noise_grid = job_inputs$y_noise,
  prior_quality = prior_quality,
  p_star_grid = job_inputs$p_star_grid,
  seeds = job_inputs$seeds,
  data_scenarios = job_inputs$data_scenarios,
  repo_root = job_inputs$repo_root,
  sampled_scenario_summary = job_inputs$sampled_scenario_summary,
  runs_per_task = job_inputs$runs_per_task,
  buffer_flush_interval = job_inputs$buffer_flush_interval,
  email = job_inputs$email,
  output_root = job_inputs$output_root,
  credible_set_rho = job_inputs$credible_set_rho,
  purity_threshold = job_inputs$purity_threshold,
  verbose_file_output = job_inputs$verbose_file_output,
  write_legacy_snp_csv = job_inputs$write_legacy_snp_csv,
  grid_mode = job_inputs$grid_mode,
  pair_L_p_star = job_inputs$pair_L_p_star,
  sigma_0_2_scalars = job_inputs$sigma_0_2_scalars,
  annotation_scales = job_inputs$annotation_scale
)
```

# 3. Inspect the Design

## Selected Use Cases

```{r preview-use-cases}
job_config$tables$use_cases %>%
  filter(use_case_id %in% job_inputs$use_cases) %>%
  select(use_case_id, label, mu_strategy, sigma_strategy, extra_compute) %>%
  kable()
```

## Scenario Grid

Each row is a unique combination of simulation settings (before seeding
and model selection).

```{r preview-scenarios}
job_config$tables$scenarios %>%
  select(scenario_id, data_scenario, L, y_noise, p_star,
         annotation_r2, inflate_match, gamma_shrink) %>%
  kable()
```

## Data Matrices Per Scenario

```{r preview-matrices}
matrix_summary <- job_config$tables$data_matrices %>%
  group_by(data_scenario) %>%
  summarise(
    n_matrices = dplyr::n(),
    participant_counts = paste(sort(unique(participant_count[!is.na(participant_count)])), collapse = ", "),
    snp_range = if (all(is.na(snps_post))) "n/a" else {
      rng <- range(snps_post, na.rm = TRUE)
      if (rng[1] == rng[2]) as.character(rng[1]) else paste(rng, collapse = " - ")
    },
    .groups = "drop"
  )
matrix_summary %>% kable()

job_config$tables$data_matrices %>%
  select(matrix_id, data_scenario, dataset_label, participant_count, snps_post, snp_set) %>%
  arrange(data_scenario, matrix_id) %>%
  slice_head(n = 10) %>%
  kable()
```

## Run Counts and Tasks

```{r preview-runs}
table_limit <- 100
run_summary <- summarise_job_config(job_config) %>%
  arrange(use_case_id, L, y_noise, p_star)

run_summary %>%
  slice_head(n = table_limit) %>%
  kable()

if (nrow(run_summary) > table_limit) {
  cat("\nShowing", table_limit, "of", nrow(run_summary), "rows (set a smaller grid or inspect the CSV for full details).\n")
}
```

```{r preview-tasks}
task_preview <- job_config$tables$tasks %>%
  arrange(task_id)

task_preview %>%
  slice_head(n = table_limit) %>%
  kable()

if (nrow(task_preview) > table_limit) {
  cat("\nShowing", table_limit, "of", nrow(task_preview), "rows.\n")
}
```

Verify that the number of runs and tasks aligns with expectations. The
total run count is:

```{r total-counts}
total_runs <- nrow(job_config$tables$runs)
total_tasks <- nrow(job_config$tables$tasks)
cat("Total model runs:", total_runs, "\n")
cat("Total SLURM tasks:", total_tasks, "\n")
```

# 4. Write Control Files

Set `commit_artifacts <- TRUE` once you are satisfied with the preview.
The chunk below writes:

1.  `output/temp/<job_name>/job_config.json` and supporting files
2.  `output/slurm_scripts/<job_name>.slurm`

During the first SLURM task, these temp files will be moved to 
`output/run_history/<job_name>/<parent_job_id>/` for permanent record-keeping.

```{r write-artifacts}
commit_artifacts <- TRUE  # flip to TRUE when ready

if (commit_artifacts) {
  artifacts <- write_job_artifacts(job_config, run_task_script = here("inst", "scripts", "run_task.R"))
  str(artifacts)
} else {
  message("Artifacts not written (commit_artifacts is FALSE).")
}
```

```{r test the first run locally}
# Define paths and arguments
run_task_script <- here("inst", "scripts", "run_task.R")
job_name <- "prior_default_testing"
task_id <- "1" # Using the first task for testing
job_root <- here("output")
# Note: config_path now points to temp folder, not run_history
config_path <- here("output", "temp", job_name, "job_config.json")

# Construct the full command
command <- sprintf(
  'Rscript "%s" --job-name "%s" --task-id "%s" --job-root "%s" --config-path "%s"',
  run_task_script,
  job_name,
  task_id,
  job_root,
  config_path
)

# Print the command to verify it's correct
print(command)

# Execute the command
system(command)
```

```{r debug run_task function}
job_name   <- "susie_scen_1_recreation"
task_id    <- 1L
job_root   <- here("output")
config_path <- here("output", "temp", job_name, "job_config.json")
quiet      <- FALSE

options(error = quote(browser(skipCalls = 1)))  # drop into the debugger on error
on.exit(options(error = NULL), add = TRUE)

debugonce(run_task)

test_susine::run_task(
  job_name   = job_name,
  task_id    = task_id,
  job_root   = job_root,
  config_path = config_path,
  quiet      = quiet
)

```

```{r run-all-tasks-locally}
run_task_script <- here("inst", "scripts", "run_task.R")
job_name <- "susie_scen_1_recreation"
job_root <- here("output")
config_path <- file.path(job_root, "temp", job_name, "job_config.json")

cfg <- jsonlite::read_json(config_path, simplifyVector = TRUE)
task_ids <- sort(unique(cfg$tables$runs$task_id))

for (task_id in task_ids) {
  cmd <- sprintf(
    'Rscript "%s" --job-name "%s" --task-id "%s" --job-root "%s" --config-path "%s"',
    run_task_script,
    job_name,
    task_id,
    job_root,
    config_path
  )
  message("→ Running task ", task_id, ": ", cmd)
  status <- system(cmd)
  if (status != 0) {
    stop("Task ", task_id, " exited with status ", status, call. = FALSE)
  }
}
```

# 5. Next Steps

1.  Inspect the generated SLURM script at
    `output/slurm_scripts/<job_name>.slurm`.

2.  Submit the array job from the cluster:

    ``` bash
    sbatch output/slurm_scripts/<job_name>.slurm
    ```

3.  Monitor progress via the structured logs in
    `output/slurm_prints/<job_name>/`.

4.  Each completed task populates
    `output/slurm_output/<job_name>/task-XXX/`, including per-run
    model/effect metrics, combined PIPs, truth tables, and saved fit
    objects (`fit.rds` plus optional `subfits.rds` for model averaging
    cases).

Happy simulating!
