---
title: "SuSiNE SNPs Dataset Preview"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: false
---

# Overview

Quick peek at the Parquet-based `snps_dataset` without loading the full table.

## Setup

```{r setup, message=FALSE}
library(here)
library(ggplot2)
here::i_am("vignettes/snps_dataset_preview.Rmd")
here()

knitr::opts_chunk$set(
  fig.width = 10,
  fig.height = 6,
  dpi = 300,
  fig.retina = 2,
  out.width = "100%"
)
```

## Load a small sample

```{r load-snps-sample, message=FALSE}
# Adjust these to point at the desired run
job_name <- "susie_scen_1_recreation"
job_id <- "46880273"

snps_path <- file.path(
  here(),
  "output", "slurm_output",
  job_name, job_id,
  "combined", "aggregated",
  "snps_dataset"
)

stopifnot(dir.exists(snps_path))

# Create output directory for plots
plot_dir <- file.path(dirname(snps_path), "plots")
dir.create(plot_dir, showWarnings = FALSE, recursive = TRUE)
message("Plots will be saved to: ", plot_dir)

library(dplyr)
library(arrow)

# Read only the first few rows; arrow will push the slice down so it does not scan the whole dataset
snps_preview <- arrow::open_dataset(snps_path) %>%
  slice_head(n = 20) %>%
  collect()

snps_preview
```

# Temporarily repartition existing dataset by use_case_id

```{r repartition-snps-dataset, message=FALSE}
# # This writes a partitioned copy alongside the original without loading everything into memory.
# # Remove or disable after use; future runs should emit partitioned data natively.
# partitioned_path <- file.path(dirname(snps_path), "snps_dataset_by_use_case")
# 
# if (!dir.exists(partitioned_path)) {
#   # Map run_id -> use_case_id (small table) then join in-arrow and write partitioned
#   run_table_path <- file.path(here(), "output", "run_history", job_name, job_id, "run_table.csv")
#   run_map <- readr::read_csv(
#     run_table_path,
#     show_col_types = FALSE,
#     col_select = c(run_id, use_case_id),
#     col_types = readr::cols(
#       run_id = readr::col_integer(),
#       use_case_id = readr::col_character()
#     )
#   ) %>%
#     dplyr::mutate(run_id = as.integer(run_id))
# 
#   snps_ds <- arrow::open_dataset(snps_path)
#   run_map_ds <- arrow::InMemoryDataset$create(run_map)
# 
#   snps_with_uc <- snps_ds %>%
#     dplyr::left_join(run_map_ds, by = "run_id") %>%
#     dplyr::select(use_case_id, dplyr::everything())
# 
#   arrow::write_dataset(
#     snps_with_uc,
#     path = partitioned_path,
#     format = "parquet",
#     partitioning = "use_case_id"
#   )
# 
#   rm(run_map, run_map_ds, snps_ds, snps_with_uc)
# }
# 
# partitioned_path
```

# Power vs. FDR (Fig. 2D/E style) from SNP PIPs

```{r power-fdr-setup}
# User inputs
split_by_var_list <- c("use_case_id") # options: use_case_id, p_star, L, y_noise, annotation_r2, inflate_match, gamma_shrink
filter_p_star <- 5                   # set to 5 to focus on p_star = 5, or NA to include all
filter_annotation_r2 <- NA            # set to 0.8 to focus on high-quality annotations (includes NA), or NA to include all
pip_bucket_width <- 0.01               # bucket width for PIP sweeping; smaller = finer curves
```

```{r power-fdr-curves, message=FALSE}
split_cols <- c("run_id", "use_case_id", "p_star", "L", "y_noise", "annotation_r2", "inflate_match", "gamma_shrink")

run_table_path <- file.path(here(), "output", "run_history", job_name, job_id, "run_table.csv")
use_cases_path <- file.path(here(), "output", "run_history", job_name, job_id, "use_cases.csv")

run_map <- readr::read_csv(
  run_table_path,
  show_col_types = FALSE,
  col_select = all_of(split_cols),
  col_types = readr::cols(
    run_id = readr::col_integer(),
    use_case_id = readr::col_character(),
    p_star = readr::col_integer(),
    L = readr::col_integer(),
    y_noise = readr::col_double(),
    annotation_r2 = readr::col_double(),
    inflate_match = readr::col_double(),
    gamma_shrink = readr::col_double()
  )
) %>%
  dplyr::mutate(run_id = as.integer(run_id))

use_case_labels <- readr::read_csv(
  use_cases_path,
  show_col_types = FALSE,
  col_select = c(use_case_id, label),
  col_types = readr::cols(
    use_case_id = readr::col_character(),
    label = readr::col_character()
  )
)

run_map <- dplyr::left_join(run_map, use_case_labels, by = "use_case_id")

# Guard split vars
group_vars <- split_by_var_list
stopifnot(length(group_vars) >= 1)
if (!all(group_vars %in% colnames(run_map))) {
  stop("split_by_var_list contains unsupported columns.")
}

# Open SNPs dataset and check what columns are already present
snps_ds <- arrow::open_dataset(snps_path)
snps_cols <- names(snps_ds)

# If use_case_id is already in the SNPs dataset (from partitioning or embedded),
# exclude it from run_map to avoid column collision during join.
# But keep 'label' since it's not in snps and we need it for plotting.
join_cols_to_exclude <- intersect(setdiff(names(run_map), c("run_id", "label")), snps_cols)
if (length(join_cols_to_exclude)) {
  message("SNPs dataset already has columns: ", paste(join_cols_to_exclude, collapse = ", "),
          " â€” excluding from join to avoid collision.")
  run_map_for_join <- dplyr::select(run_map, -all_of(join_cols_to_exclude))
} else {
  run_map_for_join <- run_map
}

run_map_ds <- arrow::InMemoryDataset$create(run_map_for_join)

snps_joined <- snps_ds %>%
  dplyr::left_join(run_map_ds, by = "run_id")

if (!is.na(filter_p_star)) {
  snps_joined <- snps_joined %>% dplyr::filter(p_star == filter_p_star)
}
if (!is.na(filter_annotation_r2)) {
  snps_joined <- snps_joined %>% dplyr::filter(annotation_r2 == filter_annotation_r2 | is.na(annotation_r2))
}

# Include label in grouping when we split by use_case_id so legends show names
label_group <- if ("use_case_id" %in% group_vars) "label" else character(0)

# Bucket PIPs once, then accumulate to build Power vs. FDR curves
pip_counts <- snps_joined %>%
  dplyr::mutate(
    pip_bucket = pmax(0, pmin(1, floor(pip / pip_bucket_width) * pip_bucket_width))
  ) %>%
  dplyr::group_by(dplyr::across(all_of(c(group_vars, label_group))), pip_bucket) %>%
  dplyr::summarise(
    n = dplyr::n(),
    pos = sum(causal == 1, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  dplyr::collect()

pip_curve_data <- pip_counts %>%
  dplyr::group_by(dplyr::across(all_of(c(group_vars, label_group)))) %>%
  dplyr::arrange(dplyr::desc(pip_bucket), .by_group = TRUE) %>%
  dplyr::mutate(
    total_pos = sum(pos, na.rm = TRUE),
    cum_tp = cumsum(pos),
    cum_pred = cumsum(n),
    precision = if_else(cum_pred > 0, cum_tp / cum_pred, NA_real_),
    recall = if_else(total_pos > 0, cum_tp / total_pos, NA_real_),
    fdr = 1 - precision
  ) %>%
  dplyr::filter(total_pos > 0, !is.na(precision), !is.na(recall)) %>%
  dplyr::ungroup()

# Build human-friendly labels: prefer use_case label when available; append other group fields
extra_vars <- setdiff(group_vars, "use_case_id")
pip_curve_data <- pip_curve_data %>%
  dplyr::mutate(
    base_label = dplyr::coalesce(label, use_case_id),
    extra_label = if (length(extra_vars)) {
      interaction(dplyr::across(all_of(extra_vars)), drop = TRUE, lex.order = TRUE)
    } else {
      NA_character_
    },
    curve_label = ifelse(is.na(extra_label), base_label, paste0(base_label, " [", extra_label, "]"))
  )

# Persist the curve data for reuse
pip_curve_data_path <- file.path(dirname(snps_path), "pip_power_fdr.csv")
readr::write_csv(pip_curve_data, pip_curve_data_path)

pip_curve_data_path
```

```{r power-fdr-plot, fig.width=15, fig.height=7, message=FALSE}
p_power_fdr <- ggplot(pip_curve_data, aes(x = fdr, y = recall, color = curve_label)) +
  geom_line() +
  geom_point(size = 0.8) +
  labs(
    title = "Power vs. FDR (PIP sweep)",
    subtitle = paste0(
      "Grouped by: ", paste(split_by_var_list, collapse = ", "),
      if (!is.na(filter_p_star)) paste0("; p_star = ", filter_p_star) else "",
      if (!is.na(filter_annotation_r2)) paste0("; annotation_r2 = ", filter_annotation_r2, " (including NA)") else ""
    ),
    x = "False Discovery Rate (1 - Precision)",
    y = "Power (Recall)",
    color = "Group"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

ggsave(file.path(plot_dir, "power_vs_fdr.png"), p_power_fdr, width = 15, height = 7, dpi = 300)
message("Saved: power_vs_fdr.png")
# p_power_fdr  # Uncomment to display in viewer
```

# Notes

- The preview uses `slice_head()` on the Arrow dataset, so it avoids loading the full Parquet collection.
- Bump `n` in `slice_head()` if you want to see more rows; keep it small to stay memory-light.

# Pre-aggregated Confusion Matrix Table

This section creates a compact summary table with TP/FP/TN/FN counts at each PIP threshold,
stratified by all grouping variables. This allows flexible downstream analysis without
loading the full SNP-level data.

```{r estimate-preagg-size, message=FALSE}
# Estimate size of pre-aggregated confusion matrix table
# --------------------------------------------------------

# Count unique values for each potential grouping variable
run_table_path <- file.path(here(), "output", "run_history", job_name, job_id, "run_table.csv")
run_map_full <- readr::read_csv(run_table_path, show_col_types = FALSE)

grouping_vars <- c("use_case_id", "p_star", "L", "y_noise", 
                   "annotation_r2", "inflate_match", "gamma_shrink")

# Count unique combinations (some will be NA for uninformed models)
unique_combos <- run_map_full %>%
  dplyr::select(all_of(grouping_vars)) %>%
  dplyr::distinct() %>%
  nrow()

# PIP thresholds (0.00, 0.01, 0.02, ..., 1.00)
pip_bucket_width <- 0.01
n_pip_thresholds <- ceiling(1 / pip_bucket_width) + 1  # 101 thresholds

# Estimated rows
n_rows <- unique_combos * n_pip_thresholds

# Columns: grouping vars + label + pip_threshold + TP + FP + TN + FN + n_runs
n_cols <- length(grouping_vars) + 1 + 1 + 4 + 1  # +1 for label, +1 for pip_threshold, +4 for counts, +1 for n_runs

# Estimate memory (rough: 8 bytes per numeric, ~50 bytes per string on average)
# Assume ~100 bytes per row as upper bound
estimated_bytes <- n_rows * 100
estimated_mb <- estimated_bytes / 1024 / 1024

cat("Grouping variables:", paste(grouping_vars, collapse = ", "), "\n")
cat("Unique setting combinations:", unique_combos, "\n")
cat("PIP thresholds:", n_pip_thresholds, "\n")
cat("Estimated rows:", format(n_rows, big.mark = ","), "\n")
cat("Estimated columns:", n_cols, "\n")
cat("Estimated memory:", round(estimated_mb, 2), "MB\n")
```

```{r build-preagg-confusion-matrix, message=FALSE}
# Build the pre-aggregated confusion matrix table
# ------------------------------------------------

grouping_vars <- c("use_case_id", "p_star", "L", "y_noise", 
                   "annotation_r2", "inflate_match", "gamma_shrink")
pip_bucket_width <- 0.01

# Load run metadata
run_table_path <- file.path(here(), "output", "run_history", job_name, job_id, "run_table.csv")
use_cases_path <- file.path(here(), "output", "run_history", job_name, job_id, "use_cases.csv")

run_map <- readr::read_csv(
  run_table_path,
  show_col_types = FALSE,
  col_select = c("run_id", all_of(grouping_vars)),
  col_types = readr::cols(
    run_id = readr::col_integer(),
    use_case_id = readr::col_character(),
    p_star = readr::col_integer(),
    L = readr::col_integer(),
    y_noise = readr::col_double(),
    annotation_r2 = readr::col_double(),
    inflate_match = readr::col_double(),
    gamma_shrink = readr::col_double()
  )
) %>%
  dplyr::mutate(run_id = as.integer(run_id))

use_case_labels <- readr::read_csv(
  use_cases_path,
  show_col_types = FALSE,
  col_select = c(use_case_id, label),
  col_types = readr::cols(
    use_case_id = readr::col_character(),
    label = readr::col_character()
  )
)

run_map <- dplyr::left_join(run_map, use_case_labels, by = "use_case_id")

# Count runs per grouping combination (for later averaging)
n_runs_per_group <- run_map %>%
  dplyr::group_by(across(all_of(grouping_vars))) %>%
  dplyr::summarise(n_runs = dplyr::n(), .groups = "drop")

# Open SNPs dataset
snps_ds <- arrow::open_dataset(snps_path)
snps_cols <- names(snps_ds)

# Prepare join table (exclude columns already in snps to avoid collision)
join_cols_to_exclude <- intersect(setdiff(names(run_map), c("run_id", "label")), snps_cols)
if (length(join_cols_to_exclude)) {
  run_map_for_join <- dplyr::select(run_map, -all_of(join_cols_to_exclude))
} else {
  run_map_for_join <- run_map
}
run_map_ds <- arrow::InMemoryDataset$create(run_map_for_join)

# Join and aggregate: count TP, FP, TN, FN at each pip_bucket for each grouping combo
# For each SNP: if pip >= threshold, it's "predicted positive"
# causal == 1 means actually positive

message("Aggregating confusion matrix counts (this may take a while)...")

confusion_agg <- snps_ds %>%

  dplyr::left_join(run_map_ds, by = "run_id") %>%
  dplyr::mutate(
    pip_bucket = pmax(0, pmin(1, floor(pip / pip_bucket_width) * pip_bucket_width))
  ) %>%
  dplyr::group_by(across(all_of(c(grouping_vars, "label"))), pip_bucket) %>%
  dplyr::summarise(
    n_snps = dplyr::n(),
    n_causal = sum(causal == 1, na.rm = TRUE),
    n_noncausal = sum(causal == 0, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  dplyr::collect()

message("Collected ", nrow(confusion_agg), " rows from Arrow")

# Store the per-bucket counts (NOT cumulative) - cumulative will be computed at query time
# This allows correct aggregation across any grouping of variables later
confusion_matrix_table <- confusion_agg %>%
  dplyr::left_join(n_runs_per_group, by = grouping_vars) %>%
  dplyr::rename(
    n_causal_at_bucket = n_causal,
    n_noncausal_at_bucket = n_noncausal
  ) %>%
  dplyr::select(
    all_of(grouping_vars), label, pip_threshold = pip_bucket,
    n_causal_at_bucket, n_noncausal_at_bucket, n_runs
  )

# Save to file
confusion_matrix_path <- file.path(dirname(snps_path), "confusion_matrix_by_threshold.csv")
readr::write_csv(confusion_matrix_table, confusion_matrix_path)
message("Saved pre-aggregated confusion matrix to: ", confusion_matrix_path)

# Show summary
cat("\nConfusion matrix table:\n")
cat("  Rows:", format(nrow(confusion_matrix_table), big.mark = ","), "\n")
cat("  Columns:", ncol(confusion_matrix_table), "\n")
cat("  Size in memory:", format(object.size(confusion_matrix_table), units = "MB"), "\n")
head(confusion_matrix_table)
```

```{r load-and-plot-power-fdr-from-preagg, message=FALSE}
# Load the pre-aggregated confusion matrix and create Power vs FDR plot
# ----------------------------------------------------------------------

# Read the saved file (simulating loading from disk)
confusion_matrix_path <- file.path(dirname(snps_path), "confusion_matrix_by_threshold.csv")
cm_table <- readr::read_csv(confusion_matrix_path, show_col_types = FALSE)

# --- USER FILTERS (adjust as needed) ---
filter_p_star_val <- 5        # NA to include all
filter_y_noise_val <- NA      # NA to include all
filter_annotation_r2_val <- NA # NA to include all (includes NA annotation runs)
filter_L_val <- NA            # NA to include all
# ----------------------------------------

# Apply filters
cm_filtered <- cm_table
if (!is.na(filter_p_star_val)) {
  cm_filtered <- cm_filtered %>% dplyr::filter(p_star == filter_p_star_val)
}
if (!is.na(filter_y_noise_val)) {
  cm_filtered <- cm_filtered %>% dplyr::filter(y_noise == filter_y_noise_val)
}
if (!is.na(filter_annotation_r2_val)) {
  cm_filtered <- cm_filtered %>% dplyr::filter(annotation_r2 == filter_annotation_r2_val | is.na(annotation_r2))
}
if (!is.na(filter_L_val)) {
  cm_filtered <- cm_filtered %>% dplyr::filter(L == filter_L_val)
}

# First aggregate the per-bucket counts across the filtered groups,
# THEN compute cumulative TP/FP (this is the key fix!)
power_fdr_data <- cm_filtered %>%
  dplyr::group_by(use_case_id, label, pip_threshold) %>%
  dplyr::summarise(
    n_causal_at_bucket = sum(n_causal_at_bucket, na.rm = TRUE),
    n_noncausal_at_bucket = sum(n_noncausal_at_bucket, na.rm = TRUE),
    n_runs = sum(n_runs, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  # Now compute cumulative values per use_case_id (descending pip)
  dplyr::group_by(use_case_id, label) %>%
  dplyr::arrange(dplyr::desc(pip_threshold), .by_group = TRUE) %>%
  dplyr::mutate(
    TP = cumsum(n_causal_at_bucket),
    FP = cumsum(n_noncausal_at_bucket),
    total_causal = sum(n_causal_at_bucket),
    total_noncausal = sum(n_noncausal_at_bucket),
    FN = total_causal - TP,
    TN = total_noncausal - FP
  ) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(
    precision = dplyr::if_else(TP + FP > 0, TP / (TP + FP), NA_real_),
    recall = dplyr::if_else(TP + FN > 0, TP / (TP + FN), NA_real_),
    fdr = 1 - precision,
    # Average credible set size at this threshold (predicted positives per run)
    avg_cs_size = (TP + FP) / n_runs
  ) %>%
  dplyr::filter(!is.na(precision), !is.na(recall))

# Build subtitle
subtitle_parts <- c()
if (!is.na(filter_p_star_val)) subtitle_parts <- c(subtitle_parts, paste0("p* = ", filter_p_star_val))
if (!is.na(filter_y_noise_val)) subtitle_parts <- c(subtitle_parts, paste0("y_noise = ", filter_y_noise_val))
if (!is.na(filter_annotation_r2_val)) subtitle_parts <- c(subtitle_parts, paste0("annotation_r2 = ", filter_annotation_r2_val))
if (!is.na(filter_L_val)) subtitle_parts <- c(subtitle_parts, paste0("L = ", filter_L_val))
subtitle_str <- if (length(subtitle_parts)) paste(subtitle_parts, collapse = ", ") else "All settings"

# Plot Power vs FDR, colored by use_case_id (label)
p_preagg_fdr <- ggplot(power_fdr_data, aes(x = fdr, y = recall, color = label)) +
  geom_line() +
  geom_point(size = 0.8) +
  labs(
    title = "Power vs. FDR (from pre-aggregated confusion matrix)",
    subtitle = subtitle_str,
    x = "False Discovery Rate (1 - Precision)",
    y = "Power (Recall)",
    color = "Model"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

ggsave(file.path(plot_dir, "power_vs_fdr_from_preagg.png"), p_preagg_fdr, width = 12, height = 7, dpi = 300)
message("Saved: power_vs_fdr_from_preagg.png")
# p_preagg_fdr  # Uncomment to display
```

```{r preagg-informed-model-comparison, fig.width=14, fig.height=9, message=FALSE}
# Replicate the informed model comparison plot from visualize_results_workbook
# using the pre-aggregated confusion matrix instead of model_metrics
# -----------------------------------------------------------------------------

# Read the saved file
confusion_matrix_path <- file.path(dirname(snps_path), "confusion_matrix_by_threshold.csv")
cm_table <- readr::read_csv(confusion_matrix_path, show_col_types = FALSE)

# Filter to informed models + baseline (a_i)
# Focus on p_star = 1 to match the visualization workbook
informed_cm <- cm_table %>%
  dplyr::filter(use_case_id %in% c("a_i", "b_i", "b_ii", "b_iii")) %>%
  dplyr::filter(p_star == 1)

# For AUPRC calculation, we need to integrate precision over recall
# We'll compute AUPRC per (use_case_id, y_noise, annotation_r2, inflate_match, gamma_shrink)

compute_auprc <- function(df) {
  # Sort by recall ascending, then integrate (trapezoidal rule)
  df <- df %>% dplyr::arrange(recall)
  if (nrow(df) < 2) return(NA_real_)
  # Remove NAs
  df <- df %>% dplyr::filter(!is.na(recall), !is.na(precision))
  if (nrow(df) < 2) return(NA_real_)
  
  # Anchor at recall=0 with precision=1 (standard AUPRC convention)
  # This ensures curves that start at high recall (confident models) are properly credited
  if (min(df$recall) > 0) {
    df <- dplyr::bind_rows(
      tibble::tibble(recall = 0, precision = 1),
      df
    )
  }
  
  # Trapezoidal integration
  auprc <- sum(diff(df$recall) * (head(df$precision, -1) + tail(df$precision, -1)) / 2)
  auprc
}

# First aggregate per-bucket counts within each grouping, then compute cumulative TP/FP
informed_metrics <- informed_cm %>%
  dplyr::group_by(use_case_id, label, y_noise, annotation_r2, inflate_match, gamma_shrink, pip_threshold) %>%
  dplyr::summarise(
    n_causal_at_bucket = sum(n_causal_at_bucket, na.rm = TRUE),
    n_noncausal_at_bucket = sum(n_noncausal_at_bucket, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  # Compute cumulative values per group (descending pip)
  dplyr::group_by(use_case_id, label, y_noise, annotation_r2, inflate_match, gamma_shrink) %>%
  dplyr::arrange(dplyr::desc(pip_threshold), .by_group = TRUE) %>%
  dplyr::mutate(
    TP = cumsum(n_causal_at_bucket),
    FP = cumsum(n_noncausal_at_bucket),
    total_causal = sum(n_causal_at_bucket),
    FN = total_causal - TP,
    precision = dplyr::if_else(TP + FP > 0, TP / (TP + FP), NA_real_),
    recall = dplyr::if_else(TP + FN > 0, TP / (TP + FN), NA_real_)
  ) %>%
  dplyr::ungroup() %>%
  dplyr::filter(!is.na(precision), !is.na(recall))

# Group and compute AUPRC
informed_auprc <- informed_metrics %>%
  dplyr::group_by(use_case_id, label, y_noise, annotation_r2, inflate_match, gamma_shrink) %>%
  dplyr::summarise(
    AUPRC = compute_auprc(dplyr::pick(precision, recall)),
    .groups = "drop"
  ) %>%
  dplyr::mutate(
    gamma_label = dplyr::if_else(is.na(gamma_shrink), "none", sprintf("gamma=%.2f", gamma_shrink))
  )

# Expand baseline (a_i) across all annotation combinations so it appears in each facet
priors_grid <- informed_auprc %>%
  dplyr::filter(!is.na(annotation_r2) & !is.na(inflate_match)) %>%
  dplyr::distinct(annotation_r2, inflate_match)

baseline_rows <- informed_auprc %>% dplyr::filter(use_case_id == "a_i")

if (nrow(baseline_rows) > 0 & nrow(priors_grid) > 0) {
  baseline_expanded <- baseline_rows %>%
    dplyr::select(-annotation_r2, -inflate_match) %>%
    tidyr::crossing(priors_grid)
  
  combined_auprc <- dplyr::bind_rows(
    informed_auprc %>% dplyr::filter(use_case_id != "a_i"),
    baseline_expanded
  )
} else {
  combined_auprc <- informed_auprc
}

# Plot AUPRC vs y_noise, faceted by prior settings
if (!nrow(combined_auprc) ||
    !any(stats::complete.cases(combined_auprc[c("annotation_r2", "inflate_match")]))) {
  warning("No annotated informed-model rows to plot; skipping.")
} else {
  p_informed_preagg <- ggplot(combined_auprc, aes(x = y_noise, y = AUPRC, color = label, linetype = gamma_label)) +
    geom_line() +
    geom_point() +
    facet_grid(inflate_match ~ annotation_r2, 
               labeller = label_bquote(rows = inflate: .(inflate_match), cols = annotation_r2: .(annotation_r2))) +
    labs(title = "AUPRC for Informed Models (from pre-aggregated SNPs): p* = 1",
         subtitle = "Computed by integrating precision-recall curve from confusion matrix",
         x = "y_noise (Noise Level)",
         y = "AUPRC",
         color = "Model",
         linetype = "Variance shrinkage") +
    theme_minimal() +
    theme(legend.position = "right")
  
  ggsave(file.path(plot_dir, "auprc_informed_from_preagg.png"), p_informed_preagg, width = 14, height = 9, dpi = 300)
  message("Saved: auprc_informed_from_preagg.png")
  # p_informed_preagg  # Uncomment to display
}
```
