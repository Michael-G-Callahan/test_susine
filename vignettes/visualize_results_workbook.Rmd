---
title: "SuSiNE Simulation Results: Model Performance Comparison"
subtitle: "Comparing uninformed, annotation-informed, and compute-boosted SuSiNE models"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    theme: default
    code_folding: hide
---

# Overview

This workbook visualizes performance comparisons across the full suite of SuSiNE model variants from our simulation study:

-   Uninformed models: With no prior information, using naive or empirical Bayes strategies for `mu_0` and `sigma_0^2`, which approach is best?
-  Annotation-informed models: Leveraging functional annotation data to inform priors on effect sizes (through the mean, variance or both), which approach is best? How do they fair over different prior qualities?
-   Compute-boosted models: Using techniques like tempering/annealing or model averaging to improve inference, how much do these strategies help?


## Preliminaries: packages amd data

```{r setup, message=FALSE}
library(here)
here::i_am("vignettes/visualize_results_workbook.Rmd")
here()
knitr::opts_chunk$set(
  fig.width = 12,   # width in inches (increase if your plots are wide)
  fig.height = 8,   # height in inches
  dpi = 300,        # resolution
  fig.retina = 2,   # effective pixel scaling for retina
  out.width = "100%" # scale in HTML output
)
```

```{r load-data, message=FALSE}
# INPUTS
job_name = "susine_sim_informed_grid"
job_id = "46208799"

# Load necessary libraries
library(dplyr)
library(readr)
library(ggplot2)

# Define file paths
run_table_path <- file.path(here(), "output", "run_history", job_name, job_id, "run_table.csv")
use_cases_path <- file.path(here(), "output", "run_history", job_name, job_id, "use_cases.csv")
effect_metrics_path <- file.path(here(), "output", "slurm_output", job_name, job_id, "combined", "effect_metrics_combined.csv")
model_metrics_path <- file.path(here(), "output", "slurm_output", job_name, job_id, "combined", "model_metrics_combined.csv")

# Load the data
run_table <- read_csv(run_table_path)
use_cases <- read_csv(use_cases_path)
effect_metrics <- read_csv(effect_metrics_path)
model_metrics <- read_csv(model_metrics_path)

# Create full_run_settings
full_run_settings <- left_join(run_table, use_cases, by = "use_case_id")

# Join metrics with run settings
# Rename use_case_id in model_metrics to avoid conflict
model_metrics_renamed <- model_metrics %>%
  rename(use_case_id_metric = use_case_id)
model_metrics_full <- left_join(model_metrics_renamed, full_run_settings, by = "run_id")

effect_metrics_renamed <- effect_metrics %>%
    rename(use_case_id_metric = use_case_id)
effect_metrics_full <- left_join(effect_metrics_renamed, full_run_settings, by = "run_id")

# Display the first few rows of the joined tables
head(model_metrics_full)
head(effect_metrics_full)
```

## Compared amongst uninformed models

```{r uninformed-model-comparison}
# Filter for uninformed models and average over seeds
# compute the average AUPRC and the average log cross-entropy (mean of log of each seed's cross_entropy)
eps <- 1e-12
uninformed_metrics_summary <- model_metrics_full %>%
  filter(use_case_id %in% c("a_i", "a_ii", "a_iii", "a_iv")) %>%
  group_by(use_case_id, label, p_star, y_noise, L) %>%
  summarise(
    AUPRC = mean(AUPRC, na.rm = TRUE),
    log_cross_entropy = mean(log(cross_entropy + eps), na.rm = TRUE),
    .groups = 'drop'
  )

# Plot AUPRC vs y_noise, faceted by L and p_star
ggplot(uninformed_metrics_summary, aes(x = y_noise, y = AUPRC, color = label)) +
  geom_line() +
  geom_point() +
  facet_grid(L ~ p_star, labeller = label_bquote(rows = L: .(L), cols = p_star: .(p_star))) +
  labs(title = "AUPRC for Uninformed Models (Averaged over seeds)",
       x = "y_noise (Noise Level)",
       y = "Average AUPRC",
       color = "Use Case") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

This grid also lets us isolate the gains from the heavier-compute variants.

```{r compute-scaled-comparison}
compute_metrics_summary <- model_metrics_full %>%
  filter(use_case_id %in% c("a_i", "c_i", "c_ii")) %>%
  group_by(use_case_id, label, p_star, y_noise, L) %>%
  summarise(
    AUPRC = mean(AUPRC, na.rm = TRUE),
    .groups = 'drop'
  )

ggplot(compute_metrics_summary, aes(x = y_noise, y = AUPRC, color = label)) +
  geom_line() +
  geom_point() +
  facet_grid(L ~ p_star, labeller = label_bquote(rows = L: .(L), cols = p_star: .(p_star))) +
  labs(
    title = "AUPRC for Baseline vs. Compute-Heavy Models",
    subtitle = "Faceted by L (rows) and prior size p_star (columns); averaged over seeds",
    x = "y_noise (Noise Level)",
    y = "Average AUPRC",
    color = "Use Case"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

```{r informed-model-comparison}
# Filter and summarise informed models (and baseline a_i) for L = p_star = 5
eps <- 1e-12
informed_metrics_raw <- model_metrics_full %>%
  filter(use_case_id %in% c("a_i", "b_i", "b_ii", "b_iii", "b_iv")) %>%
  filter(L == 5, p_star == 5)

informed_metrics_summary <- informed_metrics_raw %>%
  group_by(use_case_id, label, p_star, y_noise, L, annotation_r2, inflate_match, gamma_shrink) %>%
  summarise(
    AUPRC = mean(AUPRC, na.rm = TRUE),
    log_cross_entropy = mean(log(cross_entropy + eps), na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  mutate(
    gamma_label = if_else(is.na(gamma_shrink), "none", sprintf("gamma=%.2f", gamma_shrink))
  )

# Expand baseline (a_i) across all non-NA annotation combinations so it appears in each non-NA facet
priors_grid <- informed_metrics_summary %>%
  filter(!is.na(annotation_r2) & !is.na(inflate_match)) %>%
  distinct(annotation_r2, inflate_match)

baseline_rows <- informed_metrics_summary %>% filter(use_case_id == "a_i")

if(nrow(baseline_rows) > 0 & nrow(priors_grid) > 0) {
  # replicate baseline across prior grid while preserving y_noise and other grouping vars
  baseline_expanded <- baseline_rows %>%
    select(-annotation_r2, -inflate_match) %>%
    tidyr::crossing(priors_grid)

  combined_metrics <- bind_rows(
    informed_metrics_summary %>% filter(use_case_id != "a_i"),
    baseline_expanded
  )
} else {
  combined_metrics <- informed_metrics_summary
}

# Plot AUPRC vs y_noise, faceted by prior noise settings (rows = noncausal, cols = causal)
ggplot(combined_metrics, aes(x = y_noise, y = AUPRC, color = label, linetype = gamma_label)) +
  geom_line() +
  geom_point() +
  facet_grid(inflate_match ~ annotation_r2, labeller = label_bquote(rows = inflate: .(inflate_match), cols = annotation_r2: .(annotation_r2))) +
  labs(title = "AUPRC for Informed Models (L = p_star = 5, Averaged over seeds)",
       x = "y_noise (Noise Level)",
       y = "Average AUPRC",
       color = "Use Case",
       linetype = "Variance shrinkage") +
  theme_minimal() +
  theme(legend.position = "right")
```

```{r informed-model-annotation-quality-grid, fig.width=10, fig.height=6}
# Focus on a representative noise level
target_noise <- 0.9
plot_title <- sprintf("AUPRC vs. Annotation R^2 (at y_noise = %.2f)", target_noise)

informed_metrics_fixed_noise_raw <- informed_metrics_raw %>%
  filter(y_noise == target_noise)

# Baseline (a_i) averaged over seeds to keep a single horizontal reference
baseline_auprc <- informed_metrics_summary %>%
  filter(y_noise == target_noise, use_case_id == "a_i") %>%
  summarise(AUPRC = mean(AUPRC, na.rm = TRUE), .groups = "drop") %>%
  pull(AUPRC)

# Helper to summarise mean AUPRC per annotation/prior setting
summarise_annotations <- function(df) {
  df %>%
    group_by(label, annotation_r2, inflate_match, gamma_shrink) %>%
    summarise(AUPRC = mean(AUPRC, na.rm = TRUE), .groups = "drop")
}

# Data for main facets (models with defined gamma_shrink)
plot_summary <- informed_metrics_fixed_noise_raw %>%
  filter(!is.na(gamma_shrink)) %>%
  summarise_annotations()

# Replicate the NA-gamma model across each facet so its line is drawn everywhere
overlay_summary <- tibble()
overlay_data <- informed_metrics_fixed_noise_raw %>%
  filter(is.na(gamma_shrink) & use_case_id != "a_i")

if (nrow(overlay_data) && length(unique(plot_summary$gamma_shrink))) {
  overlay_summary <- purrr::map_df(unique(plot_summary$gamma_shrink), function(gamma) {
    overlay_data %>%
      mutate(gamma_shrink = gamma)
  }) %>%
    summarise_annotations()
}

# Plot AUPRC vs annotation_r2, faceted by inflate_match and gamma_shrink
annotation_breaks <- sort(unique(plot_summary$annotation_r2))

plot_obj <- ggplot(plot_summary, aes(x = annotation_r2, y = AUPRC, color = label, group = label)) +
  geom_hline(
    data = tibble(baseline_auprc = baseline_auprc),
    aes(yintercept = baseline_auprc, linetype = "Baseline (Uninformed)"),
    color = "red"
  ) +
  geom_line() +
  geom_point()

if (nrow(overlay_summary)) {
  plot_obj <- plot_obj +
    geom_line(data = overlay_summary, aes(group = label), linetype = "solid", alpha = 0.8) +
    geom_point(data = overlay_summary, alpha = 0.8)
}

plot_obj +
  facet_grid(
    gamma_shrink ~ inflate_match,
    labeller = label_bquote(rows = gamma == .(gamma_shrink), cols = inflate == .(inflate_match))
  ) +
  scale_x_continuous(breaks = annotation_breaks, labels = scales::number_format(accuracy = 0.01)) +
  labs(
    title = plot_title,
    subtitle = "Faceted by annotation noise parameters; lines show mean over seeds.",
    x = "Annotation R^2 (Causal Variance Explained)",
    y = "AUPRC",
    color = "Model",
    linetype = ""
  ) +
  scale_linetype_manual(values = c("Baseline (Uninformed)" = "dashed")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```
