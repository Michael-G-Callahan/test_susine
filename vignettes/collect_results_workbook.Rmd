---
title: "SuSiNE Simulation Results Collection"
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
editor_options:
  markdown:
    wrap: 72
---

```{r setup, message=FALSE}
here::i_am("vignettes/collect_results_workbook.Rmd")
library(here)
here()

library(devtools)
devtools::load_all(".")

library(dplyr)
library(tidyr)
library(readr)
library(knitr)
library(purrr)
library(fs)
library(arrow)
```

# Results Collection Workflow

This workbook validates task-level staging outputs, then aggregates them
into job-level tables outside of the staging folder.

# 1. Specify the Job

Provide the job name and parent array ID from your completed run.

```{r job-inputs}
job_inputs <- list(
  # Job identifier (from run_history folder name)
  job_name = "susie_sim_grid_demo",
  
  # Parent SLURM array job ID (from run_history/job_name/[PARENT_ID]/)
  parent_job_id = "45832058",
  
  # Root output directory
  output_root = here("output")
)
```

# 2. Load Run Table and Paths

Read the run configuration, establish paths, and confirm that staging
exists.

```{r setup-paths}
run_history_dir <- file.path(
  job_inputs$output_root,
  "run_history",
  job_inputs$job_name,
  job_inputs$parent_job_id
)

results_dir <- file.path(
  job_inputs$output_root,
  "slurm_output",
  job_inputs$job_name,
  job_inputs$parent_job_id
)

staging_dir <- file.path(results_dir, "combined", "staging")
combined_dir <- file.path(results_dir, "combined", "aggregated")

dir.create(combined_dir, showWarnings = FALSE, recursive = TRUE)

if (!dir.exists(run_history_dir)) {
  stop("Run history directory not found: ", run_history_dir)
}
if (!dir.exists(results_dir)) {
  stop("Results directory not found: ", results_dir)
}
if (!dir.exists(staging_dir)) {
  stop("Staging directory not found: ", staging_dir)
}

run_table <- read_csv(
  file.path(run_history_dir, "run_table.csv"),
  show_col_types = FALSE
)

job_config_path <- file.path(run_history_dir, "job_config.json")
if (!file.exists(job_config_path)) {
  stop("Job configuration not found: ", job_config_path)
}
job_config <- test_susine:::load_job_config(job_config_path)
verbose_file_output <- isTRUE(job_config$job$verbose_file_output)

cat("Loaded run_table with", nrow(run_table), "runs\n")
cat("Verbose file output:", verbose_file_output, "\n")
cat("Staging dir:", staging_dir, "\n")
```

# 3. Index Task/Flush Outputs

List every file written by each task/flush and get a quick glimpse at the
coverage.

```{r index-staging}
staging_index <- test_susine::index_staging_outputs(
  job_name = job_inputs$job_name,
  parent_job_id = job_inputs$parent_job_id,
  output_root = job_inputs$output_root
)

cat("Discovered", nrow(staging_index), "staging files across",
    length(unique(staging_index$task_id)), "tasks\n")

kable(head(staging_index, 10), caption = "Sample of staging files")
```

# 4. Validate Staging Files

Ensure each CSV/Parquet is readable before aggregation.

```{r validate-staging}
validation <- test_susine::validate_staging_outputs(staging_index)

if (nrow(validation)) {
  fail_count <- sum(!validation$ok, na.rm = TRUE)
  cat("Validation failures:", fail_count, "\n")
  if (fail_count > 0) {
    print(dplyr::filter(validation, !ok))
    stop("Fix unreadable staging files before aggregating.")
  }
} else {
  cat("No staging files found to validate.\n")
}
```

# 5. Aggregate to Job-Level Outputs

Combine all task/flush artifacts into consolidated tables outside of
the staging directory.

```{r aggregate}
agg <- test_susine::aggregate_staging_outputs(
  job_name = job_inputs$job_name,
  parent_job_id = job_inputs$parent_job_id,
  output_root = job_inputs$output_root,
  validate = FALSE,  # already validated above
  output_dir = combined_dir
)

cat("Aggregated outputs written to:", agg$output_dir, "\n")


if (dir.exists(file.path(agg$output_dir, "snps_dataset"))) {
  snp_files <- list.files(file.path(agg$output_dir, "snps_dataset"), pattern = "\\.parquet$", recursive = TRUE)
  cat("SNP dataset fragments:", length(snp_files), "\n")
}

# 6. Next Steps

Use the aggregated outputs in downstream analysis or dashboards. The
staging folder remains untouched; you can delete it after confirming the
aggregated files if storage is tight.

# 7. (Optional) Clean Up Staging

Once you trust the aggregated outputs, remove the task/flush staging folders
to reclaim space. **Run only after verifying aggregation.**

```{r cleanup-staging, eval=FALSE}
unlink(staging_dir, recursive = TRUE)
cat("Deleted staging directory:", staging_dir, "\n")
```
