---
title: "SuSiNE Simulation Results Collection"
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
editor_options:
  markdown:
    wrap: 72
---

# Section 1: Setup (Always Run First)
 
Load libraries, specify job identifiers, and establish paths. This section
must be run before either Section 2 or Section 3.

```{r setup, message=FALSE}
here::i_am("vignettes/collect_results_workbook.Rmd")
library(here)
here()

library(devtools)
devtools::load_all(".")

library(dplyr)
library(tidyr)
library(readr)
library(knitr)
library(purrr)
library(fs)
library(arrow)
```

## 1.1 Specify the Job

```{r job-inputs}
job_name <- "susie_scen_1_recreation"
parent_job_id <- "46880273"
output_root <- here("output")
```

## 1.2 Resolve Paths and Load Run Table

```{r load-paths-and-run-table}
# Resolve all standard paths
paths <- test_susine::resolve_job_paths(
  job_name = job_name,
  parent_job_id = parent_job_id,
  output_root = output_root
)

# Validate that run_history exists (required for all workflows)
if (!dir.exists(paths$run_history_dir)) {
  stop("Run history directory not found: ", paths$run_history_dir)
}

# Load run table (needed by both Section 2 and Section 3)
run_table <- readr::read_csv(
  file.path(paths$run_history_dir, "run_table.csv"),
  show_col_types = FALSE
)

# Use cases path (for labels in confusion matrices)
use_cases_path <- file.path(paths$run_history_dir, "use_cases.csv")

cat("Job:", job_name, "/", parent_job_id, "\n")
cat("Run history dir:", paths$run_history_dir, "\n")
cat("Aggregated dir:", paths$aggregated_dir, "\n")
cat("SNPs dataset dir:", paths$snps_dataset_dir, "\n")
cat("Loaded run_table with", nrow(run_table), "runs\n")
```

---
---

# Section 2: Staging → Aggregated (Auto-skip if already done)

This section processes raw staging outputs into aggregated files.
It will **automatically skip** if aggregated outputs already exist.

```{r check-aggregation-status}
staging_exists <- dir.exists(paths$staging_dir)
aggregated_exists <- dir.exists(paths$snps_dataset_dir) && 
                     length(list.files(paths$snps_dataset_dir, recursive = TRUE)) > 0

cat("Staging directory exists:", staging_exists, "\n")
cat("Aggregated SNPs dataset exists:", aggregated_exists, "\n")

# Determine whether to run aggregation
run_aggregation <- staging_exists && !aggregated_exists

if (aggregated_exists) {
  message("✓ Aggregated outputs already exist. Skipping Section 2.")
} else if (!staging_exists) {
  stop("Neither staging nor aggregated outputs found. Cannot proceed.")
} else {
  message("→ Staging found but not yet aggregated. Will run aggregation.")
}
```

## 2.1 Index Staging Files

```{r index-staging}
if (run_aggregation) {
  staging_index <- test_susine::index_staging_outputs(
    job_name = job_name,
    parent_job_id = parent_job_id,
    output_root = output_root
  )
  
  cat("Discovered", nrow(staging_index), "staging files across",
      length(unique(staging_index$task_id)), "tasks\n")
} else {
  message("Skipped: aggregation not needed.")
}
```

## 2.2 Validate Staging Files

```{r validate-staging}
if (run_aggregation) {
  validation <- test_susine::validate_staging_outputs(staging_index)
  
  if (nrow(validation)) {
    fail_count <- sum(!validation$ok, na.rm = TRUE)
    cat("Validation failures:", fail_count, "\n")
    if (fail_count > 0) {
      print(dplyr::filter(validation, !ok))
      stop("Fix unreadable staging files before aggregating.")
    }
  } else {
    cat("No staging files found to validate.\n")
  }
} else {
  message("Skipped: aggregation not needed.")
}
```

## 2.3 Aggregate to Job-Level Outputs

```{r aggregate}
if (run_aggregation) {
  dir.create(paths$aggregated_dir, showWarnings = FALSE, recursive = TRUE)
  
  agg <- test_susine::aggregate_staging_outputs(
    job_name = job_name,
    parent_job_id = parent_job_id,
    output_root = output_root,
    validate = FALSE,  # already validated above
    output_dir = paths$aggregated_dir
  )
  
  cat("Aggregated outputs written to:", agg$output_dir, "\n")
  
  if (dir.exists(paths$snps_dataset_dir)) {
    snp_files <- list.files(paths$snps_dataset_dir, pattern = "\\.parquet$", recursive = TRUE)
    cat("SNP dataset fragments:", length(snp_files), "\n")
  }
} else {
  message("Skipped: aggregation not needed.")
}
```

## 2.4 (Optional) Delete Staging - MANUAL ONLY

**This chunk is disabled by default for safety.** Set `eval=TRUE` manually
if you want to delete staging files after confirming aggregation.

```{r cleanup-staging, eval=FALSE}
# DANGER: Set eval=TRUE only after verifying aggregation is complete
if (dir.exists(paths$staging_dir)) {
  unlink(paths$staging_dir, recursive = TRUE)
  cat("Deleted staging directory:", paths$staging_dir, "\n")
} else {
  cat("Staging directory already deleted.\n")
}
```

---
---

# Section 3: Build Pre-aggregated Confusion Matrices

Create compact summary tables for Power/FDR/AUPRC analysis. This section
requires only the aggregated SNPs dataset (not staging files).

**Prerequisites:** 
- Section 1 must have been run (to load paths and run_table)
- SNPs dataset must exist at `paths$snps_dataset_dir`

## 3.1 Check SNPs Dataset Exists

```{r check-snps-dataset}
snps_exists <- dir.exists(paths$snps_dataset_dir)
cat("SNPs dataset exists:", snps_exists, "\n")
if (!snps_exists) {
  stop("SNPs dataset not found at: ", paths$snps_dataset_dir,
       "\nRun Section 2 first to aggregate staging outputs.")
}

# Show partitions
partitions <- list.dirs(paths$snps_dataset_dir, recursive = FALSE, full.names = FALSE)
cat("Partitions found:", length(partitions), "\n")
if (length(partitions) <= 10) {
  cat(paste(" ", partitions, collapse = "\n"), "\n")
}
```

## 3.2 Configure Confusion Matrix Parameters

```{r confusion-config}
# PIP bucket width (0.01 = 100 buckets from 0 to 1)
pip_bucket_width <- 0.01

# Grouping variables for the detailed confusion matrix
# This will create one row per (grouping combo × pip_threshold)
detailed_grouping_vars <- c(
  "use_case_id", "p_star", "L", "y_noise",
  "annotation_r2", "inflate_match", "gamma_shrink"
)

# Grouping variable for the dataset-level confusion matrix
dataset_grouping_vars <- c("matrix_id")
```

## 3.3 Build Detailed Confusion Matrix (by Use Case + Parameters)

```{r build-confusion-detailed, message=FALSE}
detailed_output_path <- file.path(paths$aggregated_dir, "confusion_matrix_by_use_case.csv")

confusion_detailed <- test_susine::build_confusion_matrix(
  snps_dataset_path = paths$snps_dataset_dir,
  run_table = run_table,
  grouping_vars = detailed_grouping_vars,
  pip_bucket_width = pip_bucket_width,
  use_cases_path = use_cases_path,
  output_path = detailed_output_path
)

cat("\nDetailed confusion matrix:\n")
cat("  Rows:", format(nrow(confusion_detailed), big.mark = ","), "\n")
cat("  Columns:", ncol(confusion_detailed), "\n")
cat("  Size in memory:", format(object.size(confusion_detailed), units = "MB"), "\n")
cat("  Saved to:", detailed_output_path, "\n")
```

## 3.4 Build Dataset-Level Confusion Matrix (by matrix_id only)

```{r build-confusion-dataset, message=FALSE}
dataset_output_path <- file.path(paths$aggregated_dir, "confusion_matrix_by_dataset.csv")

confusion_by_dataset <- test_susine::build_confusion_matrix(
  snps_dataset_path = paths$snps_dataset_dir,
  run_table = run_table,
  grouping_vars = dataset_grouping_vars,
  pip_bucket_width = pip_bucket_width,
  use_cases_path = NULL,  
  output_path = dataset_output_path
)

cat("\nDataset-level confusion matrix:\n")
cat("  Rows:", format(nrow(confusion_by_dataset), big.mark = ","), "\n")
cat("  Unique matrix_ids:", dplyr::n_distinct(confusion_by_dataset$matrix_id), "\n")
cat("  Size in memory:", format(object.size(confusion_by_dataset), units = "KB"), "\n")
cat("  Saved to:", dataset_output_path, "\n")
```

## 3.5 Summary

```{r summary}
cat("\n=== Output Files ===\n")
output_files <- list.files(paths$aggregated_dir, pattern = "\\.(csv|parquet)$", recursive = FALSE)
for (f in output_files) {
  fpath <- file.path(paths$aggregated_dir, f)
  fsize <- file.size(fpath)
  cat(sprintf("  %s (%s)\n", f, format(structure(fsize, class = "object_size"), units = "auto")))
}

if (dir.exists(paths$snps_dataset_dir)) {
  snp_size <- sum(file.size(list.files(paths$snps_dataset_dir, recursive = TRUE, full.names = TRUE)))
  cat(sprintf("  snps_dataset/ (%s total)\n", format(structure(snp_size, class = "object_size"), units = "auto")))
}
```
